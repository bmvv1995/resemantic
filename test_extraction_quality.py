#!/usr/bin/env python3
"""
Test extraction quality pe conversaÈ›ii simulate
GenereazÄƒ raport detaliat pentru analizÄƒ
"""

import json
import sys
from datetime import datetime
from pathlib import Path

# Add parent to path
sys.path.insert(0, str(Path(__file__).parent))

from config import Neo4jConfig, SQLiteConfig
from neo4j import GraphDatabase
import sqlite3

# Test conversations
CONVERSATIONS = {
    "technical_deepdive": [
        {"role": "user", "content": "Cum configurez un warehouse pentru textile Ã®n ERP?"},
        {"role": "assistant", "content": "Pentru warehouse textile ai nevoie de: 1) Zone de stocare separate (bumbac, sintetice, mixte), 2) Reguli de inventar specifice (FIFO pentru sezoniere), 3) Labels cu compoziÈ›ie materialÄƒ. Hai sÄƒ vedem pas cu pas..."},
        {"role": "user", "content": "Am nevoie de integrare cu Shopify pentru sync real-time?"},
        {"role": "assistant", "content": "Da, pentru sync real-time recomand: webhook-uri Shopify pentru comenzi noi, API pentru update stoc, È™i queue system (Redis) pentru retry la erori de network. Alternativa e polling la 5 min, dar pierde comenzi Ã®n rush hours."},
        {"role": "user", "content": "Ok, mergem cu webhooks. Cum handling erorilor?"},
        {"role": "assistant", "content": "Pentru webhooks: 1) Retry exponential (2^n seconds, max 3 attempts), 2) Dead letter queue pentru failed events, 3) Monitoring cu alerting la >5% failure rate. Critic: webhooks trebuie sÄƒ rÄƒspundÄƒ Ã®n <5s sau Shopify le marcheazÄƒ failed."},
        {"role": "user", "content": "Perfect. Deci decizia: webhooks cu retry logic È™i DLQ. ImplementÄƒm?"},
        {"role": "assistant", "content": "Da! Rezumat decizie: Webhooks pentru real-time sync, exponential retry cu max 3 attempts, DLQ pentru failures, monitoring activ. Reasoning: Real-time > polling pentru customer experience, retry logic previne data loss, DLQ asigurÄƒ recovery. Ãncepem cu webhook endpoint setup."}
    ],
    
    "problem_solving": [
        {"role": "user", "content": "Stage 1 extraction ia 6 secunde per mesaj. E prea mult!"},
        {"role": "assistant", "content": "6s e mult. Posibile cauze: 1) Prompt size mare (~1500 tokens), 2) Model rÄƒspuns lent (network?), 3) JSON parsing complex. Hai sÄƒ verificÄƒm prompt size primul."},
        {"role": "user", "content": "Da, promptul e lung. Are decoraÈ›ii È™i exemple multe."},
        {"role": "assistant", "content": "Exact! DecoraÈ›ii = waste tokens. SoluÈ›ii: A) Compact prompt (400 tokens vs 1500), B) System message (sent once), C) Few-shot minimal (2 examples vs 10). Recomand: B+C = ~70% token reduction."},
        {"role": "user", "content": "Dar quality nu scade cu prompt mai scurt?"},
        {"role": "assistant", "content": "Risc valid! Trade-off: Tokensâ†“ = Latencyâ†“ + Costâ†“, DAR Qualityâ†“?. Mitigare: A/B test (20 msgs cu fiecare), comparÄƒ quality manual. DacÄƒ quality OK â†’ keep compact. DacÄƒ degradeazÄƒ â†’ keep verbose dar add caching."},
        {"role": "user", "content": "Ok, facem A/B test. Deci decizia: testÄƒm compact prompt, validÄƒm quality, apoi deploy."},
        {"role": "assistant", "content": "Perfect! Decision logged: Compact prompt cu A/B validation. Reasoning: Performance gain justificÄƒ test, dar quality e gatekeeper. Next: Implement compact_v2 cu LangSmith tags pentru compare."}
    ],
    
    "mixed_context": [
        {"role": "user", "content": "Salut! Hai sa discutÄƒm despre ReSemantic graph structure."},
        {"role": "assistant", "content": "BunÄƒ! Cu plÄƒcere. ReSemantic foloseÈ™te graph nativ: nodes = propositions, edges = temporal (NEXT) + semantic (COHERENT). Ce te intereseazÄƒ specific?"},
        {"role": "user", "content": "De ce edges ponderate, nu simple connections?"},
        {"role": "assistant", "content": "Weights = semantic similarity scores (0.4-1.0). Permit: 1) Traversal inteligent (follow doar >0.7 pentru high-confidence), 2) Sleep cycles (degrade weak edges), 3) Retrieval rankning. Simple edges = all equal, pierde nuanÈ›Äƒ."},
        {"role": "user", "content": "Ce mai faci apropo?"},
        {"role": "assistant", "content": "Bine, mulÈ›umesc! Tu cum te simÈ›i cu proiectul?"},
        {"role": "user", "content": "Ok, ok. Deci la edges, cum calculating similarity?"},
        {"role": "assistant", "content": "Cosine similarity pe embeddings (1536-dim). Formula: cos(A,B) = AÂ·B / (||A|| ||B||). Result âˆˆ [-1,1], dar folosim doar [0.4, 1.0] pentru positive correlation. Sub 0.4 = no edge (noise)."},
        {"role": "user", "content": "MulÈ›umesc, e clar acum!"},
        {"role": "assistant", "content": "Cu plÄƒcere! DacÄƒ mai ai Ã®ntrebÄƒri, sunt aici."}
    ]
}

def analyze_extraction_quality(conversation_name, messages):
    """RuleazÄƒ extraction È™i analizeazÄƒ quality"""
    
    print(f"\n{'='*80}")
    print(f"ğŸ”¬ ANALYZING: {conversation_name}")
    print(f"{'='*80}\n")
    
    # Aici ar rula extraction real prin graph
    # Pentru acum, simulÄƒm sÄƒ vedem structura raportului
    
    print(f"ğŸ“Š CONVERSATION STATS:")
    print(f"   Total messages: {len(messages)}")
    print(f"   User messages: {sum(1 for m in messages if m['role'] == 'user')}")
    print(f"   Assistant messages: {sum(1 for m in messages if m['role'] == 'assistant')}")
    
    # Message length distribution
    lengths = [len(m['content']) for m in messages]
    print(f"\nğŸ“ MESSAGE LENGTHS:")
    print(f"   Min: {min(lengths)} chars")
    print(f"   Max: {max(lengths)} chars")
    print(f"   Avg: {sum(lengths)/len(lengths):.0f} chars")
    
    # Simulate extraction results
    print(f"\nğŸ§ª EXTRACTION SIMULATION:")
    print(f"   (Run actual extraction to populate this)")
    print(f"   Expected propositions: ~{len(messages) * 3} (avg 3 per msg)")
    print(f"   Expected concepts: ??? (to analyze)")
    print(f"   Expected decisions: ??? (to track)")
    
    return {
        "name": conversation_name,
        "message_count": len(messages),
        "avg_length": sum(lengths)/len(lengths)
    }

def main():
    print("ğŸ§ª RESEMANTIC EXTRACTION QUALITY TEST")
    print("="*80)
    
    results = []
    
    for name, messages in CONVERSATIONS.items():
        result = analyze_extraction_quality(name, messages)
        results.append(result)
    
    print(f"\n\nğŸ“‹ SUMMARY:")
    print(f"{'='*80}")
    for r in results:
        print(f"   {r['name']:20} | {r['message_count']:2} msgs | {r['avg_length']:.0f} chars avg")
    
    print(f"\nâœ… Next: Implementare extraction real È™i analiza output!")

if __name__ == "__main__":
    main()
