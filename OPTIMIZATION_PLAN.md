# ğŸš€ Stage 1 Optimization Plan

## PROBLEMA IDENTIFICATÄ‚
- Prompt prea verbos: ~1500 tokens
- Latency: 5-7s per mesaj
- 58% din total extraction time

## SOLUÈšIE: Prompt Compact + System Message

### ÃNAINTE (1500 tokens):
```python
prompt = f"""TU EÈ˜TI UN ANALIZOR...
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TASK: ... (10 linii)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
EXEMPLE: ... (8 linii)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
...
"""
```

### DUPÄ‚ (400 tokens):
```python
# System message (se trimite o singurÄƒ datÄƒ)
system = """Analizor conversaÈ›ii: extrage semantic unit JSON.
Type: question/statement/command
Certainty: high/medium/low  
Narrative_role: core/supportive/peripheral"""

# User prompt (compact)
prompt = f"""Context: {context}
Message ({role}, {timestamp}): {message}

JSON:
{{
  "unit_id": "{id}",
  "content": "descriere mesaj",
  "type": "...",
  "concepts": [...],
  "certainty": "...",
  "narrative_role": "..."
}}"""
```

## BENEFICII AÈ˜TEPTATE:
- âš¡ Tokens: -70% (1500 â†’ 450)
- âš¡ Latency: -50% (6s â†’ 3s)
- âš¡ Cost: -70%
- âœ… Quality: SAME (esenÈ›a e pÄƒstratÄƒ)

## IMPLEMENTARE:

### OpÈ›iunea 1: System Message Ã®n ChatAnthropic
```python
llm = ChatAnthropic(
    model="claude-3-5-haiku-20241022",
    system=SYSTEM_PROMPT,  # â† Trimis o singurÄƒ datÄƒ
    temperature=0.3
)

# Ãn node:
messages = [{"role": "user", "content": compact_prompt}]
response = llm.invoke(messages)
```

### OpÈ›iunea 2: Few-Shot Examples Ã®n System
```python
SYSTEM_PROMPT = """Analizor semantic.

Exemple:
Input: "buna claude"
Output: {"content": "User salutÄƒ", "type": "greeting", ...}

Input: "cum configurez warehouse?"  
Output: {"content": "User Ã®ntreabÄƒ despre configurare warehouse", "type": "question", ...}

Extrage similar pentru orice mesaj."""
```

### OpÈ›iunea 3: Prompt Template Optimizat
```python
COMPACT_TEMPLATE = """
Context: {context}
Msg: {message}

Extract JSON: unit_id, content, type, concepts, certainty, narrative_role
"""
```

## TESTARE A/B:
1. Tag runs cu "prompt_version": "v1_verbose" / "v2_compact"
2. RuleazÄƒ 20 mesaje cu fiecare
3. ComparÄƒ Ã®n LangSmith:
   - Latency
   - Quality (manual check propositions)
   - Token count

## ROLLBACK STRATEGY:
DacÄƒ quality degradeazÄƒ â†’ revert la verbose
DacÄƒ quality OK â†’ keep compact, reduce cu Ã®ncÄƒ 20%

## NEXT LEVEL (dupÄƒ success):
- Cache system prompt (Anthropic Prompt Caching)
- Batch multiple messages Ã®n 1 request
- Parallel Stage1 user + assistant
